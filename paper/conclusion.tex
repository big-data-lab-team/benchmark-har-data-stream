\section{Conclusion}

We conclude that the \hoeffdingtree, the \mondrianforest, and the
\naivebayes data stream classifiers have an overall superiority over the
\FNN and the \mcnns ones for \har.  However, the prediction performance
remains quite low compared to an offline \knn classifier, and it varies
substantially between datasets. Noticeably, the \hoeffdingtree and the
\mcnns classifiers are more resilient to concept drift that the other ones.

Regarding memory consumption, only the \mcnn and \naivebayes classifiers
were found to have a negligible memory footprint, in the order of a few
kilobytes, which is compatible with connected objects. Conversely, the
memory consumed by a \mondrianforest, a \FNN or a \hoeffdingtree is in the
order of 100~kB, which would only be available on some connected objects.
In addition, the classification performance of a \mondrianforest is
strongly modulated by the amount of memory allocated. With enough memory, a
\mondrianforest is likely to match or exceed the performance of the
\hoeffdingtree and \naivebayes classifiers.

The amount of energy consumed by classifiers is mostly impacted by their
runtime, as all power consumptions were found comparable. The
\hoeffdingtree and \mondrianforest are substantially slower than the other
classifiers, with runtimes in the order of 0.35~ms/element, a performance not compatible 
with common sampling frequencies of wearable sensors. 

Future research will focus on reducing the memory requirements and runtime
of the \hoeffdingtree and the \mondrianforest classifiers. In addition to
improving the deployability of these classifiers on connected objects, this
would also potentially improve their classification performance, since
memory remains a bottleneck in the \mondrianforest.

\section*{Acknowledgement}
This work was funded by a Strategic Project Grant of the Natural Sciences
and Engineering Research Council of Canada. The computing platform was
obtained with funding from the Canada Foundation for Innovation.

% vim: tw=80 ts=2

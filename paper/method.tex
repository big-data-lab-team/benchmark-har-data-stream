\section{Method}
We evaluated 5 classifiers implemented in either
StreamDM-Cpp~\cite{StreamDM-CPP} or
OrpailleCC~\cite{OrpailleCC}.  StreamDM-Cpp is a
C++ implementation of StreamDM~\cite{StreamDM}, a
software to mine big data streams using
\href{https://spark.apache.org/streaming/}{Apache
Spark Streaming}. StreamDM-Cpp is usually faster
than StreamDM in single-core environments, due to the
overhead induced by Spark.

OrpailleCC is a collection of data stream
algorithms developed for embedded devices. The key
functions, such as random number generation or
memory allocation, are parametrizable through
class templates and can thus be customized on a
given execution platform.  OrpailleCC is not
limited to classification algorithms, it
implements other data stream algorithms such as
the Cuckoo filter~\cite{cuckoo} or a
multi-dimensional extension of the Lightweight
Temporal Compression~\cite{multi-ltc}. We extended
it with a few classifiers for the purpose of this
benchmark.

This benchmark includes five popular classification
algorithms.  The
\mondrianforest~\cite{mondrian2014} builds
decision trees without immediate need for labels
which is useful in situations where labels are
delayed~\cite{stream_learning_review}.  The
Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the k-nearest
neighbor~(KNN) that was shown to be among the most accurate classifiers for human activity
recognition from wearable sensors~\cite{Janidarmian_2017}. The \naivebayes~\cite{naive_bayes}
classifier builds a table of feature
occurrence to estimate class
likelihoods.
The \hoeffdingtree~\cite{VFDT} builds a
decision tree using the Hoeffding Bound to
estimate when the best split is found. 
Finally, Neural Network classifiers have
become popular by reaching or even exceeding human performance in many fields such as image
recognition or game playing. We used a
\FNN with one hidden layer, as described in~\cite{omid_2019} for the recognition 
of fitness activities on a low-power platform.

The remainder of this section details the datasets, classifiers,
evaluation metrics and parameters used in our benchmark.

\subsection{Datasets}
\label{sec:method-dataset}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a
human activity dataset with 17 participants
and 9 sensors per
participant\footnote{\banosdataset dataset
available
\href{https://archive.ics.uci.edu/ml/datasets/REALDISP+Activity+Recognition+Dataset\#:\~:text=The\%20REALDISP\%20(REAListic\%20sensor\%20DISPlacement,\%2Dplacement\%20and\%20induced\%2Ddisplacement.}{here}.}. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as
well as the orientation in a quaternion format,
producing a total of 13 values.  Sensors are
sampled at 50~Hz, and each sample is associated
with one of 33 activities. In addition to the 33
activities, an extra activity labelled 0 indicates
no specific activity.

We pre-processed the \banosdataset dataset using
non-overlapping windows of one second (50
samples), and using only the 6 acceleration axes
of the right forearm sensor. We computed the average and the standard deviation over the
window as features for each axis. We assigned the most
frequent label to the window.  The resulting data
points were shuffled uniformly.

In addition, we constructed another dataset from \banosdataset, in which we
simulated a concept drift by shifting the activity labels in the
second half of the data stream. This is useful to
observe any behavioral change induced by the
concept drift such as an increase in power
consumption.

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset containing 94
participants\footnote{\recofitdataset dataset
available
\href{https://msropendata.com/datasets/799c1167-2c8f-44c4-929c-227bf04e2b9a}{here}.}. Similarly to the \banosdataset
dataset, the activity labelled 0 indicates no
specific activity.
Since many of these activities are similar, we
merged some of them together based on the table
in~\cite{behzad2019}. 

We pre-processed the dataset similarly to the
\banosdataset one, using non-overlapping windows of
one second, and only using the 6 acceleration axes
of the 6 axis data. From these 6 axes, we used the average and the standard deviation
over the window as features. We assigned the most
frequent label to the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We generated three synthetic datasets\footnote{MOA commands available \href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}.}:
a hyperplane, a RandomRBF, and a RandomTree
dataset. We generated 200,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF both have three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three, with six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.

\subsection{Algorithms and Implementation}
In this section, we describe the algorithms used in the benchmark, their
hyperparameters, and relevant implementation details. 

\subsubsection{\mondrianforest~\cite{mondrian2014}}
Each tree in a \mondrianforest recursively splits
the feature space, similar to a regular decision tree.
However, the feature used in the split and the
value of the split are picked randomly. The
probability to select a feature is proportional to
its normalized range. The value for the split is
uniformly selected in the range of the feature.
During prediction, a node combines its observed
label count with its parent prediction.

In OrpailleCC, the size allocated for the forest
is set beforehand and it is shared by all
the trees.  Therefore, the memory footprint of the
classifier is constant.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

Table~\ref{table:hyperparameter-mondrian} shows
the hyperparameters used for each number of trees.
\begin{figure}
	\begin{center}
		\begin{tabular}{|| r | c | c | c ||} 
			\hline
			Number of trees &  Base count & Discount & Budget \\ [0.5ex] 
			\hline\hline
			1 & 0.0 & 1.0 & 1.0 \\
			5 & 0.0 & 1.0 & 0.4 \\
			10 & 0.0 & 1.0 & 0.4 \\
			50 & 0.0 & 1.0 & 0.2 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Hyperparameters used for \mondrianforest.}
		\label{table:hyperparameter-mondrian}
\end{figure}
\subsubsection{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(\mcnn) is a
variant of k-nearest neighbors where data points
are aggregated into clusters to reduce storage
requirements.  During training, the algorithm
merges a new data point to the closest cluster
that shares the same label. If the closest cluster
does not share the same label as the data point,
this closest cluster and the closest cluster with
the same label are assigned an error. When a
cluster receives too many errors, it is split.
During classification, \mcnn returns the label of
the closest cluster.  Regularly, the algorithm
also assigns a participation score to each cluster
and when this score gets below a threshold, the
cluster is removed. Given that the maximum number
of clusters is fixed, this mechanism makes
space for new clusters, and possibly
helps adjust to concept drifts.  
The space and time complexity of \mcnn are
constant since the maximum number of clusters does
not change.

We implemented two versions of \mcnn in
OrpailleCC, which differ in the way they remove
clusters during training. The first version (\mcnn
Origin) is similar to the mechanism described
in~\cite{mc-nn}, based on participation scores.
The second version (\mcnn OrpailleCC)
removes the cluster with the lowest participation
only when space is needed.  A cluster slot is
needed when an existing cluster is split and there
are no more slot available because the number of
active clusters already reached the maximum defined
by the user.

\mcnn OrpailleCC has one
parameter, the error threshold after which a
cluster is split.  \mcnn Origin has two
parameters: the error threshold and the
participation threshold. The participation
threshold is the limit below which a cluster is
removed.

Table~\ref{table:hyperparameter-mcnn} shows
the hyperparameters used for each number of clusters \TG{where is the participation threshold?}.
\begin{figure}
		\begin{center}
			\begin{tabular}{|| r | c ||} 
				\hline
				Number of clusters &  error threshold \\ [0.5ex] 
				\hline\hline
				10 & 2 \\
				20 & 10 \\
				33 & 16 \\
				40 & 8 \\
				50 & 2 \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Hyperparameters used for \mcnn.}
		\label{table:hyperparameter-mcnn}
\end{figure}

\subsubsection{\naivebayes~\cite{naive_bayes}}
The \naivebayes algorithm keeps a table of
counters for each feature value and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase.

The implementation from StreamDM was used in this
benchmark. It uses a Gaussian
fit for numerical attributes.

In a \naivebayes classifier, the smoothing parameter is the
default counter given to an attribute value that
has not been seen. It is meant to avoid scores of zeros.
We do not use \TG{could you check that all the verbs that describe the experiment are present tense? 
A few of them are past.} any smoothing since our datasets
only contain numerical
attributes. Therefore, as long as there is one data
point to train with, there will be a Gaussian
fit for each attribute.

\subsubsection{\hoeffdingtree~\cite{VFDT}}
%\begin{itemize}
	%\item Reserved size with a given size.
	%\item Binary tree.
	%\item Focus on real numbers features.
	%\item The number of split considered by features is given by the user.
	%\item Split are determined by forming a boxes and spliting these boxes.
	%\item Majority vote at the leaves.
	%\item All floating point values are double and all counters are int.
%\end{itemize}
Similar to a decision tree, the \hoeffdingtree
recursively splits the feature space to maximize a metric, often the
information gain or the Gini
index. However,  to estimate when a leaf should be
split, the \hoeffdingtree relies on the
Hoeffding bound, a measure of the score deviation
of the splits instead of using the entire
dataset \TG{unclear: what is using the entire dataset? decision trees only focus on the data in the leaf to split}. During classification, a data point
is sorted to a leaf, and a label is predicted by
aggregating the labels of the training data points
in that leaf, usually through majority voting or
\naivebayes classification.  We used this
classifier as implemented in StreamDM-Cpp.  The
\hoeffdingtree is common in data stream
classification, however, it suffers from one main
disadvantage: once a split is decided, it cannot
be re-considered which makes this algorithm weak
to concept drifts \TG{This is a weird comment given that Hoeffing Tree is
the only classifier that recovers well from the drift in the results. Also, there's no mentioning of 
concept drifts in the other classifiers, maybe just add a paragraph to comment in general about their 
robustness to concept drift.}.

The \hoeffdingtree has multiple \TG{say three explicitly} parameters: the
confidence level, the grace period, and the leaf
learner. The confidence level is probability that
the Hoeffding bound makes a wrong estimation of
the deviation. The grace period is the number of
processed data points before a leaf is evaluated for split.
 The leaf learner is the method used in the
leaf to predict the label.  In this study we used
a confidence level of $0.01$ with a grace period
of 10 data points and \naivebayes as leaf
learner.

\subsubsection{\FNN}
%We need a citation there
A neural network is a combination of artificial
neurons, also known as perceptrons, that all have input weights and an
activation function. To predict a class label, the
perceptron applies the activation function to the weighted sum
of its input values. The output
value of the perceptron is the result of this
activation function. This prediction phase is also
called feed-forward. To train the neural network,
feed-forward is applied first, then the error between the
prediction and the expected result is used in the
backpropagation process to adjust the weights of
the input values.  A neural network combines
multiple perceptrons by connecting perceptron outputs
to inputs of other perceptrons.  In
this benchmark, we used a fully-connected \FNN, 
that is, a network where perceptrons are organized in
layers and all output
values from perceptrons of layer $n-1$ serve as
input values for perceptrons of layer $n$. 
We used a 3-layer network with 120 inputs, 30
perceptrons in the hidden layer, and 33 output
perceptrons.

In this study, we used histogram features
from~\cite{omid_2019} instead of the ones
presented in Section~\ref{sec:method-dataset}
because the network performed
poorly with these features. The histogram features
produce 20 bins per axis.

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.

\subsubsection{Hyperparameters Tuning}
For each classifier, hyperparameters were tuned using the first
subject from the \banosdataset dataset.  The data from
this subject was pre-processed as the rest of
the \banosdataset dataset (window size of one second,
average and standard deviation on the three
acceleration axis of the right forearm sensor,
$\ldots$). We tested multiple values for the
parameters \TG{grid search?}.

The
classifiers start the prequential phase with no
knowledge from the first subject.
We made an exception for the \FNN because we noticed that it performed poorly with random weights and it
needed many epochs to achieve better performances
than a random  classifier. Therefore, we decided to
pre-train the \FNN and re-use
the weights as a starting point for the
prequential phase.

For other classifiers, only the 
hyperparameters were taken from the tuning phase.
We selected the hyperparameters that 
maximized the F1-score on the first subject.


\subsection{Evaluation}
We computed four metrics: the F1-score, the memory
footprint, the runtime, and the power usage.
The F1-score and the memory
footprint were computed periodically during the
execution of a classifier. The
power consumption and the runtime were collected
at the end of each execution.

%Time to process one element.
%Explain the concept drift recovery time.
\paragraph{Classification Performance}
We monitored the true positives, false positives,
true negatives, and false negatives using the
prequential evaluation, meaning that with each
new data point the model was first tested and then trained.
From these counts, we computed the F1-score every
50 elements. We did not apply any fading factor
to attenuate errors throughout the stream.
We computed the F1-score in a one-versus-all
fashion for each class, averaged across all
classes
(macro-average, code available \href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{here}). 
When a class had not been encountered yet, its F1-score was ignored. We used
the F1-score rather than the accuracy because the real data sets are
imbalanced.

\paragraph{Memory}
We measured the memory footprint by reading file
\texttt{/proc/self/statm} every 50 data points.

\paragraph{Runtime}
The runtime of a classifier is the time needed for
the classifier to process the dataset. It is
measured at the end of an execution using the
\textit{perf}
command \TG{maybe naive but how do you measure runtime only at the end of
the execution? Maybe reword that.} \TG{There is still an issue with verb
tenses here: ``It is measured'' in this paragraph, ``We measured'' in the
previous one.}. \TG{Add link to the tool}

%\MK{Explain the runtime somewhere and how it is
%computed.}

\paragraph{Power} We measured the average power
consumed by classification algorithms using the
\textit{perf} command. We measured the power used by each
classifier multiple times in a minimal
environment. To provide a baseline power usage, we
also added an empty classifier that always
predicted class 0.

\subsection{Experimental Conditions}
We automated our experiments with a Python script that defines
classifiers and their parameters, randomizes all
the repetitions, and plots the
resulting data. The datasets and output results are stored in memory
through a memfs filesystem mounted on \texttt{/tmp}, to reduce the impact of I/O time.
We averaged metrics accross repetitions (same classifier, same parameters, and
same dataset).

The experiment was done  with a single core of a
cluster node with two Intel(R) Xeon(R)
Gold 6130 CPUs and a main memory of 250G.

% vim: tw=50 ts=2

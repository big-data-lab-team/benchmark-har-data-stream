\section{Materials and Methods}
We evaluate 5 classifiers implemented in either \streamdmcpp~\cite{StreamDM-CPP}
or OrpailleCC~\cite{OrpailleCC}.  \streamdmcpp is a C++ implementation of
StreamDM~\cite{StreamDM}, a software to mine big data streams using
\href{https://spark.apache.org/streaming/}{Apache Spark Streaming}. \streamdmcpp
is usually faster than StreamDM in single-core environments, due to the overhead
induced by Spark.

OrpailleCC is a collection of data stream algorithms developed for embedded
devices. The key functions, such as random number generation or memory
allocation, are parametrizable through class templates and can thus be
customized on a given execution platform.  OrpailleCC is not limited to
classification algorithms, it implements other data stream algorithms such as
the Cuckoo filter~\cite{cuckoo} or a multi-dimensional extension of the
Lightweight Temporal Compression~\cite{multi-ltc}. We extended it with a few
classifiers for the purpose of this benchmark.

This benchmark includes five popular classification algorithms.  The
Mondrian forest~(\mondrianforest)~\cite{mondrian2014} builds decision trees without immediate need
for labels, which is useful in situations where labels are
delayed~\cite{stream_learning_review}.  The Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the k-nearest neighbor~(\knn)
that was shown to be among the most accurate classifiers for \har from wearable
sensors~\cite{Janidarmian_2017}. The \naivebayes~\cite{naive_bayes} classifier
builds a table of attribute occurrence to estimate class likelihoods.  The
\hoeffdingtree~\cite{VFDT} builds a decision tree using the Hoeffding Bound to
estimate when the best split is found.  Finally, Neural Network classifiers have
become popular by reaching or even exceeding human performance in many fields
such as image recognition or game playing. We use a Feedforward Neural
Network~(\FNN) with one hidden layer,
as described in~\cite{omid_2019} for the recognition of fitness activities on a
low-power platform.

The remainder of this section details the datasets, classifiers, evaluation
metrics and parameters used in our benchmark.

\subsection{Datasets}
\label{sec:method-dataset}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a
human activity dataset with 17 participants
and 9 sensors per
participant\footnote{\banosdataset dataset
available
\href{https://archive.ics.uci.edu/ml/datasets/REALDISP+Activity+Recognition+Dataset\#:\~:text=The\%20REALDISP\%20(REAListic\%20sensor\%20DISPlacement,\%2Dplacement\%20and\%20induced\%2Ddisplacement.}{here}.}. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as
well as the orientation in a quaternion format,
producing a total of 13 values.  Sensors are
sampled at 50~Hz, and each sample is associated
with one of 33 activities. In addition to the 33
activities, an extra activity labeled 0 indicates
no specific activity.

We pre-process the \banosdataset dataset using
non-overlapping windows of one second (50
samples), and using only the 6 axes (acceleration
and gyroscope)
of the right forearm sensor. We compute the average and the standard deviation over the
window as features for each axis. We assign the most
frequent label to the window.  The resulting data
points were shuffled uniformly.

In addition, we construct another dataset from \banosdataset, in which we
simulate a concept drift by shifting the activity labels in the
second half of the data stream. This is useful to
observe any behavioral change induced by the
concept drift such as an increase in power
consumption.

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset containing 94
participants\footnote{\recofitdataset dataset
available
\href{https://msropendata.com/datasets/799c1167-2c8f-44c4-929c-227bf04e2b9a}{here}.}. Similarly to the \banosdataset
dataset, the activity labeled 0 indicates no
specific activity.
Since many of these activities were similar, we
merged  some of them together based on the table
in~\cite{behzad2019}. 

We pre-processed the dataset similarly to the
\banosdataset one, using non-overlapping windows of
one second, and only using 6 axes (acceleration
and gyroscope) from one sensor.
. From these 6 axes, we used the average and the standard deviation
over the window as features. We assigned the most
frequent label to the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We generate three synthetic datasets\footnote{MOA commands available \href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}.}:
a hyperplane, a RandomRBF, and a RandomTree
dataset. We generate 200,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF both have three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three, with six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.

\subsection{Algorithms and Implementation}
In this section, we describe the algorithms used in the benchmark, their
hyperparameters, and relevant implementation details. 

\subsubsection{Mondrian forest~(\mondrianforest)~\cite{mondrian2014}}
Each tree in a \mondrianforest recursively splits
the feature space, similar to a regular decision tree.
However, the feature used in the split and the
value of the split are picked randomly. The
probability to select a feature is proportional to
its normalized range, and the value for the split is
uniformly selected in the range of the feature.

%Bounded memory discussion
In OrpailleCC, the amount of memory allocated to the forest is predefined,
and it is shared by all the trees in the forest, leading to a constant
memory footprint for the classifier. This implementation is memory-bounded,
meaning that the classifier can adjust to memory limitations, for instance
by stopping tree growth or replacing existing nodes with new ones. This is
different from an implementation with a constant space complexity, where
the classifier would use the same amount of memory regardless of the
amount of available memory. For instance, in our study, the \mondrianforest
classifier is memory-bounded while \naivebayes classifier has a constant
space complexity.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

Hyperparameters used for \mondrianforest are available in the
\href{https://github.com/azazel7/paper-benchmark/blob/master/README.md}{repository readme}.
Additionally, the \mondrianforest is allocated
with 600~KB of memory unless specified otherwise.
On the \banosdataset and \recofitdataset datasets,
we also explore the \mondrianforest with 3~MB of
memory in order to observe the effect of available memory on
performances (classification, runtime, and power).
\subsubsection{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(\mcnn) is a
variant of k-nearest neighbors where data points
are aggregated into clusters to reduce storage
requirements.  
The space and time complexities of \mcnn are
constant since the maximum number of clusters is fixed.
The reaction to concept drift is influenced by the
participation threshold and the error threshold.
A higher participation threshold and a lower error
threshold increase reaction speed to concept
drift. Since the error thresholds used in this
study are small, we expect \mcnn to react quite
fast and efficiently to concept drifts.

We implemented two versions of \mcnn in
OrpailleCC, differing in the way they remove
clusters during training. The first version (\mcnn
Origin) is similar to the mechanism described
in~\cite{mc-nn}, based on participation scores.
The second version (\mcnn OrpailleCC)
removes the cluster with the lowest participation
only when space is needed.  A cluster slot is
needed when an existing cluster is split and there
is no more slot available because the number of
active clusters already reached the maximum defined
by the user.

\mcnn OrpailleCC has only one
parameter, the error threshold after which a
cluster is split.  \mcnn Origin has two
parameters: the error threshold and the
participation threshold. The participation
threshold is the limit below which a cluster is
removed.
Hyperparameters used for \mcnn are available in the
\href{https://github.com/azazel7/paper-benchmark/blob/master/README.md}{repository readme}.

\subsubsection{Na√Øve Bayes~(\naivebayes)~\cite{naive_bayes}}
The \naivebayes algorithm keeps a table of
counters for each feature value and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase. 

The implementation from \streamdmcpp was used in this
benchmark. It uses a Gaussian
fit for numerical attributes. Two implementations were used, the OrpailleCC one and the StreamDM one. We used two implementations 
to provide a comparison reference between the two libraries.

\subsubsection{Hoeffding Tree~(\hoeffdingtree)~\cite{VFDT}}
Similar to a decision tree, the \hoeffdingtree recursively splits the feature
space to maximize a metric, often the information gain or the Gini index.
However,  to estimate when a leaf should be split, the \hoeffdingtree relies on
the Hoeffding bound, a measure of the score deviation of the splits. We used
this classifier as implemented in \streamdmcpp.  The \hoeffdingtree is common in
data stream classification, however, the internal nodes are static and cannot be
re-considered. Therefore, any concept drift adaption relies on the new leaves
that will be split.

The \hoeffdingtree has three parameters: the
confidence level, the grace period, and the leaf
learner. The confidence level is the probability that
the Hoeffding bound makes a wrong estimation of
the deviation. The grace period is the number of
processed data points before a leaf is evaluated for a split.
 The leaf learner is the method used in the
leaf to predict the label.  In this study, we used
a confidence level of $0.01$ with a grace period
of 10 data points and the \naivebayes classifier as leaf
learner.

\subsubsection{Feedforward Neural Network~(\FNN)}
\label{sec:method-fnn}
%We need a citation there
A neural network is a combination of artificial neurons, also known as
perceptrons, that all have input weights and an activation function. In this
benchmark, we used a fully-connected \FNN, that is, a network where perceptrons
are organized in layers and all output values from perceptrons of layer $n-1$
serve as input values for perceptrons of layer $n$.  We used a 3-layer network
with 120 inputs, 30 perceptrons in the hidden layer, and 33 output perceptrons.
Because a \FNN takes many epochs to update and converge it barely adapts to a
concept drifts even though it trains with each new data point.

In this study, we used histogram features
from~\cite{omid_2019} instead of the ones
presented in Section~\ref{sec:method-dataset}
because the network performed
poorly with these features. The histogram features
produce 20 bins per axis.

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.

\subsubsection{Hyperparameters Tuning}
For each classifier, we tuned hyperparameters  using the first subject from the
\banosdataset dataset.  The data from this subject was pre-processed as the rest
of the \banosdataset dataset (window size of one second, average and standard
deviation on the six-axis of the right forearm sensor, $\ldots$). We did a grid
search to test multiple values for the parameters.

The classifiers start the prequential phase with
no knowledge from the first subject.  We made an
exception for the \FNN because we noticed that it
performed poorly with random weights and it needed
many epochs to achieve better performances than a
random  classifier. Therefore, we decided to
pre-train the \FNN and re-use the weights as a
starting point for the prequential phase.

For other classifiers, only the 
hyperparameters were taken from the tuning phase.
We selected the hyperparameters that 
maximized the F1 score on the first subject.

\subsubsection{Offline Comparison}
We compared data stream algorithms with an offline \knn. The value of $k$ were
selected using a grid search.

\subsection{Evaluation}
We computed four metrics: the F1 score, the memory
footprint, the runtime, and the power usage.
The F1 score and the memory
footprint were computed periodically during the
execution of a classifier. The
power consumption and the runtime were collected
at the end of each execution.

%Time to process one element.
%Explain the concept drift recovery time.
\paragraph{Classification Performance}
We monitor the true positives, false positives,
true negatives, and false negatives using the
prequential evaluation, meaning that with each new
data point the model is first tested and then
trained.  From these counts, we compute the
F1 score every 50 elements. We do not apply any
fading factor to attenuate errors throughout the
stream.  We compute the F1 score in a
one-versus-all fashion for each class, averaged
across all classes (macro-average, code available
\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{here}).
When a class has not been encountered yet, its
F1 score is ignored. We use the F1 score rather
than the accuracy because the real data sets are
imbalanced.


\paragraph{Memory}
We measure the memory footprint by reading file
\texttt{/proc/self/statm} every 50 data points.

\paragraph{Runtime}
The runtime of a classifier is the time needed for
the classifier to process the dataset. We
collect the runtime reported by the \textit{perf}
command\footnote{\textit{perf}
\href{https://perf.wiki.kernel.org/index.php/Main_Page}{website}}, which
includes loading of the binary in memory, setting up data structures, and
opening the dataset file. To remove these overheads from our measurements,
we use the runtime of an empty classifier that always predict class 0 as a baseline.

\paragraph{Power} We measure the average power consumed by classification
algorithms with the \textit{perf} command. The power measurement is done
multiple times in a minimal environment. We use the empty classifier as a
baseline.

\subsection{Experimental Conditions}
We automated our experiments with a Python script that defines
classifiers and their parameters, randomizes all
the repetitions, and plots the
resulting data. The datasets and output results were stored in memory
through a memfs filesystem mounted on \texttt{/tmp}, to reduce the impact of I/O time.
We averaged metrics across repetitions (same classifier, same parameters, and
same dataset).

The experiment was done with a single core of a
cluster node with two Intel(R) Xeon(R)
Gold 6130 CPUs and a main memory of 250G.

% vim: tw=80 ts=2

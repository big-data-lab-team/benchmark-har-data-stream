\section{Method}
We evaluated 6 classifiers implemented in
either StreamDM-Cpp~\cite{StreamDM-CPP} or
OrpailleCC~\cite{OrpailleCC}.  StreamDM-Cpp is a
C++ implementation of StreamDM~\cite{StreamDM}, a
software to mine big data streams using Spark
Streaming \TG{isn't the official name Apache Spark Streaming? You should also add a URL.}. StreamDM-Cpp is usually faster than
StreamDM on small datasets, due to
the overhead induced by Spark, but it makes heavy
use of the C standard library, in particular the vector and
list structures, which may not be available on embedded systems
 \TG{This should be explained in intro rather than here.}.

OrpailleCC is a
collection of data stream algorithms developed for embedded devices. The key
functions, such as random number generation or memory allocation, are
parametrizable through class templates and can thus be customized 
on a given execution platform.
OrpailleCC is not limited to classification
algorithms, it implements other data stream
algorithms such as the Cuckoo filter~\cite{cuckoo}
or a multi-dimensional extension of the
Lightweight Temporal Compression~\cite{multi-ltc}. We extended it with 
a few classifiers for the purpose of this benchmark.

This benchmark includes six popular classification algorithms.  The Mondrian
forest~\cite{mondrian2014} builds decision trees independently of class labels,
and only assign class labels to tree leaves\TG{reworded this but it's still a bit ambiguous}. Mondrian forests are useful in
situations where labels are delayed, as described
in~\cite{stream_learning_review}.  The Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the K-nearest
neighbor~(KNN) that was shown to be more efficient for human activity
recognition \TG{reference? efficient $\rightarrow$ accurate?}. We included
it since the work in~\cite{Janidarmian_2017} showed that KNN was the
best-performing classifier for human activity recognition from wearable
sensors.  The Naïve Bayes~\cite{naive_bayes} classifier
and the Hoeffding Tree~\cite{VFDT} are two common data stream classifiers \TG{Add a sentence to explain the main idea or feature or Hoeffding tree}.
Finally, Neural Network classifiers have
become popular by achieving human-like performance in many fields such as image
recognition or game playing. We used a
Multi-Layer Perceptron with one hidden layer, as described in~\cite{omid_2019} for the recognition 
of fitness activities on a low-power platform.


\TG{This is a bit too detailed here: ``with a reported F1-score of $0.95$. They used one hidden
layer with the sigmoid activation except for the
output which uses the softmax."}

The remainder of this section details the datasets, classifiers,
evaluation metrics and parameters used in our benchmark.

\subsection{Datasets}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a
human activity dataset with 17 participants
equipped with 9 sensors. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as
well as the orientation in a quaternion format,
producing a total of 13 values.  Sensors are
sampled at 50~Hz, and each sample is associated
with one of 33 activities. In addition to the 33
activities, an extra activity labelled 0 indicates
no specific activity.

We pre-processed the \banosdataset dataset using
non-overlapping windows of one second (50
samples), and using only the 6 acceleration axes
of the right forearm sensor. We computed features
as the average and the standard deviation over the
window for each axis. We assigned the most
frequent label to the window.  The resulting data
points were shuffled across subjects.

In addition, we constructed another dataset from \banosdataset, in which we
simulated a concept drift by shuffling the activity labels in the
second half of the data stream. \TG{add one sentence to explain why this is useful}

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset containing 94
participants. Similarly to the \banosdataset
dataset, the activity labelled 0 indicates no
specific activity.
Since many of these activities are similar, we
merged some of them together based on the table
povided in~\cite{behzad2019}. 

We pre-processed the dataset similarly to the
\banosdataset, using non-overlapping windows of
one second, and only using the 6 acceleration axes
of the 6 axis data. From these 6 axes, we computed
features as the average and the standard deviation
over the window. The label assigned to this data
point was the most frequent label in the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We generated three synthetic datasets\footnote{MOA commands available \href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}}:
a hyperplane, a RandomRBF, and a RandomTree
dataset \TG{juste pour mon info: tu sembles
regenerer les datasets a chaque run, utilises-tu
la meme graine a chaque fois?}. We generated 200,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF both have three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three, with six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.
\TG{You should put this dataset in the paper repo, and \banosdataset too if the license allows}

\subsection{Algorithms and Implementation}

\TG{nitpick: you should order the algorithms as they were presented in the intro of section II}

\TG{Don't use $\backslash$paragraph for chunks of text with more than 1 paragraph.
}
In this Section, we will describe algorithms uses
in this study as well as their hyperparameters. If
needed we will add implementation description.

\paragraph{Hoeffding Tree~\cite{VFDT}}
%\begin{itemize}
	%\item Reserved size with a given size.
	%\item Binary tree.
	%\item Focus on real numbers features.
	%\item The number of split considered by features is given by the user.
	%\item Split are determined by forming a boxes and spliting these boxes.
	%\item Majority vote at the leaves.
	%\item All floating point values are double and all counters are int.
%\end{itemize}
Similar to a decision tree, the Hoeffding tree recursively splits the space to maximize a
metric function, often the
information gain or the Gini index. However,
instead of using the entire dataset, the Hoeffding
tree relies on the Hoeffding bound, \TG{a measure of xxx}, to estimate
when a leaf should be split. To classify a
data point, this data point is sorted to a leaf,
and a label is predicted by aggregating the labels of the training data points in that leaf, usually through
majority voting or Naïve Bayes classification.

We used this classifier as implemented in StreamDM \TG{StreamDMCpp?}.

The Hoeffding Tree is common in data stream
classification, however, it suffers from one main
disadvantage: once a split is decided, it cannot
be re-considered which makes this algorithm weak
to concept drifts.

\paragraph{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(MCNN) is a
variant of k-nearest neighbor where data points are
aggregated into clusters to reduce storage requirements.  During training, the
algorithm merges a new data point to the closest
cluster that shares the same label. If the closest
cluster does not share the same label as the data
point, this closest cluster and the closest
cluster with the same label are assigned an error. When a cluster receives too
many errors, it is split. During classification,
MCNN returns the label of the closest cluster.
Regularly, the algorithm also assigns a
participation score to each cluster and when this
score gets below a threshold, the cluster is
removed. Given that the maximum number of clusters
is fixed, this mechanism helps to make space for
new clusters in the future, and possibly to adjust to concept drifts.  

We implemented two versions of MCNN in OrpailleCC,
which differ in the way they remove clusters
during training. The first version (MCNN Origin)
is similar to the mechanism described
in~\cite{mc-nn}, based on participation scores.
Instead, the second version (MCNN OrpailleCC)
removes the cluster with the lowest participation
only when space is needed.  A cluster slot is
needed when an old cluster is split and there are
no more slot available because the number of
active cluster already reached the maximum defined
by the user.
\TG{I
updated this paragraph, pleaase check.}

The MCNN algorithm has two
parameters: the maximum number of clusters and the
error threshold after which a cluster is split. \TG{you didn't talk about hyperparameters for hoeffding trees, make sure your descriptions are consistent.}

\TG{somewhere you should explain that your implementations work with a fixed amount of memory, maybe in the presentation of OrpailleCC at the beginning of the section.}

\paragraph{Mondrian Forest~\cite{mondrian2014}}
Each tree in a Mondrian forest recursively
splits the space, similar to a regular decision tree.
However, the feature used in the split and the value of the
split are decided randomly. The probability to select a feature is 
proportional to its normalized range. The value for the split is
uniformly selected in the range of the feature. During prediction, a node
combines its own label observation \TG{what is a node ``observation''?} with its parent prediction.

In OrpailleCC, the size allocated for the forest
is set at the beginning and it is shared by all
the trees.  Therefore, the memory footprint of the
classifier is constant.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

\paragraph{Naive Bayes~\cite{naive_bayes}}
The naive Bayes algorithm keeps a table of
counters for each feature's values and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase.

The implementation from StreamDM was used in this
benchmark. It uses a Gaussian
estimation for numeric attributes.

In a naïve Bayes classifier, the smoothing parameter is the
default counter given to an attribute value that
has not been seen. It is meant to avoid scores of zeros.
We do not use any smoothing since our datasets
only contain numeric
attributes. Therefore, as long as there is one data
point to train with, there will be a Gaussian
estimation for each attribute.

\paragraph{k-Nearest Neighbors}
\TG{did you use kNN at all? They don't seem to be in the results.}

The k-Nearest Neighbors~(kNN) is a classification
algorithm that uses the k nearest data points from
the training set to classify.  An adaption of this
algorithm to data stream classification is the use
of a sliding window~\cite{Mining_Massive_Datasets}
as the training set.

Normally, the sliding window stores the most
recent data point, but in this benchmark, we used
a biased window~\cite{biased_reservoir_sampling}.
This window acts as a sample of
the stream (similar to a reservoir sampling) but
biased on the most recent data point.  Therefore,
the window does not contain the most recent data
point, but instead, these data points will have a
higher chance of being in the window.  This type
of window is useful to keep knowledge from the
past while focusing on the recent part of the
dataset.

The kNN algorithm can be tuned by adjusting the
parameter k. It can also be tuned by changing the
distance function. Finally, it can be optimized by
adapting the window size.

\paragraph{Neural Network}
%We need a citation there
A neural network is a combination of artificial
neurons also known as perceptrons. Each
perceptron has weighted input values and an
activation function. To predict a class label, the
perceptron applies the activation function to the sum
of its input values. The output
value of the perceptron is the result of this
activation function. This prediction phase is also
called feed-forward. To train the neural network,
we feed-forward, then the error between the
prediction and the expected result is used in the
backpropagation process to adjust the weights of
the input values.  A neural network combines
multiple perceptrons by connecting perceptron outputs
to inputs of other perceptrons.  In
this benchmark, we used a Multi-Layer Perceptron, 
that is, a network where perceptrons are organized in
layers and all output
values from perceptrons of layer $n-1$ serve as
input values for perceptrons of layer $n$. 
We used a 3-layers network with 120 inputs, 30
perceptron in the hidden layer, and 33 output
perceptrons.
In this study, we used histogram features
from~\cite{omid_2019} instead of the ones
presented in Section~\ref{sec:method-dataset}
because the Lulti-Layer Perceptron would performed
poorly with the features. The histogram features
produce 20 bins per axis.

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.


\paragraph{Hyperparameters Tuning}
Hyperparameters were tuned using the first
subject from the \banosdataset dataset.  The data from
this subject was pre-processed as the rest of
the \banosdataset dataset (window size of one second,
average and standard deviation on the three
acceleration axis of the right forearm sensor,
$\ldots$). We tested multiple values for the
parameters.

Except for the Multi-Layer Perceptron, the
classifiers start the prequential phase with no
knowledge from the first subject. Only the
hyperparameters were selected to maximize the
F1-score.

On the other hand, we have noticed that the
Multi-Layer Perceptron is useless without a
pretraining of many epochs. Otherwise, it cannot
learn fast enough and it does not do better than a
random classifier. Therefore, we decided to
pre-train the Multi-Layer Perceptron and re-use
the weights as a starting point for the
prequential phase. This implies that the
Multi-Layer Perceptron starts the prequential
phase with knowledge from the data.

\subsection{Evaluation}
We used the prequential error to evaluate the algorithms \TG{it's a bit weird to have this as a first statement in this section as it only 
applies to classification performance. I'd move it to the section on classification performance.}, that is, we tested
algorithms with each new data point before training with it. When a
new label is encountered, we do \TG{fix switch between present and past} not test the algorithm with the data point, we
only train it. We did not used any fading factor in the prequential error, and we averaged
values from multiple runs \TG{what are runs? repetition with different random seeds? in different orders?}.

We computed the F1-score, the accuracy, and the memory footprint periodically
rather than after each data point, to reduce the computing load of the experiment.
We collected energy consumption at the end of each run, alongside average power.

\TG{just a matter of style, but I feel like your subsections are short enough to just be paragraphs}

%Time to process one element.
%Explain the concept drift recovery time.
\subsubsection{Classification Performance}
We used the accuracy and the F1-score to measure the classification
performances of the classifiers. These two metrics
were computed every 50
elements to avoid overloading the run with this computation \TG{so you
measure the prequential error (TP, FP, TN, FN for each class) after each
element but you compute the F1 score only every 10 elements, right? I think the prequential error should be described in this paragraph, to make it clearer.}. We
did not apply any fading factor that could attenuate errors from the
beginning.

We computed the
F1-score in a one-versus-all fashion for each class, averaged across all classes
(macro-average \TG{do you confirm?}, code available \href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{here}).  When a class has not been encountered yet, its F1-score is ignored.
Similarly to the accuracy, we did not apply a fading factor.

\subsubsection{Memory}
We measured the memory footprint by reading file
\texttt{/proc/self/statm} every 50 data points.

%\MK{Explain the runtime somewhere and how it is
%computed.}

\subsubsection{Energy}
We measured the total energy and average power consumed by classification
algorithms using the
AppPowerMeter\footnote{\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/makefile.py\#L122}{Code
available here}.} tool from rapl
tools\footnote{\url{https://github.com/kentcz/rapl-tools.git}}. We measured
the energy consumed by each classifier multiple times in a minimal
environment. To provide a baseline energy consumption, we also added an
empty classifier that always predicted class 0.

\subsection{Experimental Conditions}
We automated our experiments with a Python script that defines
classifiers and their parameters, randomizes all the runs, and plots the
resulting data. The datasets and output results are stored in memory
through a memfs filesystem mounted on \texttt{/tmp}, to reduce the impact of I/O time.

We averaged metrics for multiple runs of the same
classifier (same classifier, same parameters, and
same dataset). The energy
consumption is the average energy consumption of all runs \TG{you mean same classifier, all datasets, all parameters? If so, say it.}. Regarding the
classification performance (F1-score, accuracy), we averaged measurements
across repetitions \TG{are these repetitions the same as runs that are averaged at the beginning of section II.C? If so, 
I would remove the mentioning of averaging there, and only put it here.}.

\TG{You should describe the evaluation platform (characteristics of cluster node)}
The experiment was done  with a single core of a
cluster node. The node was two Intel(R) Xeon(R)
Gold 6130 CPU and a main memory of 250G.

% vim: tw=50 ts=2

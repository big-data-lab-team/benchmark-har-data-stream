\section{Method}
We evaluated 6 classifiers implemented in
either StreamDM-Cpp~\cite{StreamDM-CPP} or
OrpailleCC~\cite{OrpailleCC}.  StreamDM-Cpp is a
C++ implementation of StreamDM~\cite{StreamDM}, a
software to mine big data streams using Spark
Streaming \TG{isn't the official name Apache Spark Streaming? You should also add a URL.}. StreamDM-Cpp is usually faster than
StreamDM on small datasets, due to
the overhead induced by Spark, but it makes heavy
use of the C standard library, in particular the vector and
list structures, which may not be available on embedded systems
 \TG{This should be explained in intro rather than here.}.

OrpailleCC is a
collection of data stream algorithms developed for embedded devices. The key
functions, such as random number generation or memory allocation, are
parametrizable through class templates and can thus be customized 
on a given execution platform.
OrpailleCC is not limited to classification
algorithms, it implements other data stream
algorithms such as the Cuckoo filter~\cite{cuckoo}
or a multi-dimensional extension of the
Lightweight Temporal Compression~\cite{multi-ltc}. We extended it with 
a few classifiers for the purpose of this benchmark.

This benchmark includes six popular classification algorithms.  The Mondrian
forest~\cite{mondrian2014} builds decision trees independently of class labels,
and only assign class labels to tree leaves\TG{reworded this but it's still a bit ambiguous}. Mondrian forests are useful in
situations where labels are delayed, as described
in~\cite{stream_learning_review}.  The Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the K-nearest
neighbor~(KNN) that was shown to be more efficient for human activity
recognition \TG{reference? efficient $\rightarrow$ accurate?}. We included
it since the work in~\cite{Janidarmian_2017} showed that KNN was the
best-performing classifier for human activity recognition from wearable
sensors.  The Naïve Bayes~\cite{naive_bayes} classifier
and the Hoeffding Tree~\cite{VFDT} are two common data stream classifiers \TG{Add a sentence to explain the main idea or feature or Hoeffding tree}.
Finally, Neural Network classifiers have
become popular by achieving human-like performance in many fields such as image
recognition or game playing. We used a
Multi-Layer Perceptron with one hidden layer, as described in~\cite{omid_2019} for the recognition 
of fitness activities on a low-power platform.


\TG{This is a bit too detailed here: ``with a reported F1-score of $0.95$. They used one hidden
layer with the sigmoid activation except for the
output which uses the softmax."}

The remainder of this section details the datasets, classifiers,
evaluation metrics and parameters used in our benchmark.

\subsection{Datasets}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a human
activity dataset with 17 participants
equipped with 9 sensors. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as well as 
the orientation in a quaternion format, producing a total of 13 values.
Sensors are
sampled at 50~Hz, and each sample is associated
with one of 33 activities. In
addition to the 33 activities, an extra activity
labelled 0 indicates no specific activity.

We pre-processed the \banosdataset dataset using
non-overlapping windows of one second (50
samples), and using only the 3 acceleration axes of the
right forearm sensor. From each axis, features were computed as the
average and the standard deviation over the window. The label assigned to this data
point was the most frequent label in the window.
The resulting data points were shuffled
across subjects. \TG{minor comment: this paragraph switches from active to passive voice.}

In addition, we constructed another dataset from \banosdataset, in which we
simulated a concept drift by shuffling the activity labels in the
second half of the data stream. \TG{add one sentence to explain why this is useful}

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset containing 94
participants. Similarly to the \banosdataset
dataset, the activity labelled 0 indicates no
specific activity.
Since many of these activities are similar, we
merged some of them together based on the table
povided in~\cite{behzad2019}. 

We pre-processed the dataset similarly to the
\banosdataset, using non-overlapping windows of
one second, and only using the 3 acceleration axes
of the 6 axis data. From these 3 axes, we computed
features as the average and the standard deviation
over the window. The label assigned to this data
point was the most frequent label in the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We generated three synthetic datasets\footnote{MOA commands available \href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}}:
a hyperplane, a RandomRBF, and a RandomTree
dataset \TG{juste pour mon info: tu sembles regenerer les datasets a chaque run, utilises-tu la meme graine a chaque fois?}. We generated 20,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF both have three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three, with six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.
\TG{You should put this dataset in the paper repo, and \banosdataset too if the license allows}

\subsection{Algorithms and Implementation}

\TG{nitpick: you should order the algorithms as they were presented in the intro of section II}

\TG{Don't use $\backslash$paragraph for chunks of text with more than 1 paragraph.
}

\TG{It's a bit weird to have a title followed by a title: maybe add a sentence to explain that 
you will briefly explain the algorithms, review their parameters, and describe their implementation in OrpailleCC?}

\paragraph{Hoeffding Tree~\cite{VFDT}}
%\begin{itemize}
	%\item Reserved size with a given size.
	%\item Binary tree.
	%\item Focus on real numbers features.
	%\item The number of split considered by features is given by the user.
	%\item Split are determined by forming a boxes and spliting these boxes.
	%\item Majority vote at the leaves.
	%\item All floating point values are double and all counters are int.
%\end{itemize}
Similar to a decision tree, the Hoeffding tree recursively splits the space to maximize a
metric function, often the
information gain or the Gini index. However,
instead of using the entire dataset, the Hoeffding
tree relies on the Hoeffding bound, \TG{a measure of xxx}, to estimate
when a leaf should be split. To classify a
data point, this data point is sorted to a leaf,
and a label is predicted by aggregating the labels of the training data points in that leaf, usually through
majority voting or Naïve Bayes classification.

We used this classifier as implemented in StreamDM \TG{StreamDMCpp?}.

The Hoeffding Tree is common in data stream
classification, however, it suffers from one main
disadvantage: once a split is decided, it cannot
be re-considered which makes this algorithm weak
to concept drifts.

\paragraph{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(MCNN) is a
variant of k-nearest neighbor where data points are
aggregated into clusters to reduce storage requirements.  During training, the
algorithm merges a new data point to the closest
cluster that shares the same label. If the closest
cluster does not share the same label as the data
point, this closest cluster and the closest
cluster with the same label are assigned an error. When a cluster receives too
many errors, it is split. During classification,
MCNN returns the label of the closest cluster.
Regularly, the algorithm also assigns a
participation score to each cluster and when this
score gets below a threshold, the cluster is
removed. Given that the maximum number of clusters
is fixed, this mechanism helps to make space for
new clusters in the future, and possibly to adjust to concept drifts.  

We implemented two versions of MCNN in OrpailleCC, which differ in the way
they remove clusters during training. The first version (MCNN Original
\TG{could you update the name in the figure too?jour}) is similar to the
mechanism described in~\cite{mc-nn}, based on participation scores.
Instead, the second version (MCNN OrpailleCC) removes the cluster with the
lowest participation only when space is needed \TG{which is when?}. \TG{I
updated this paragraph, pleaase check.}

The MCNN algorithm has two
parameters: the maximum number of clusters and the
error threshold after which a cluster is split. \TG{you didn't talk about hyperparameters for hoeffding trees, make sure your descriptions are consistent.}

\TG{somewhere you should explain that your implementations work with a fixed amount of memory, maybe in the presentation of OrpailleCC at the beginning of the section.}

\paragraph{Mondrian Forest~\cite{mondrian2014}}
Each tree in a Mondrian forest recursively
splits the space, similar to a regular decision tree.
However, the feature used in the split and the value of the
split are decided randomly. The probability to select a feature is 
proportional to its normalized range. The value for the split is
uniformly selected in the range of the feature. During prediction, a node
combines its own label observation \TG{what is a node ``observation''?} with its parent prediction.

In OrpailleCC, the size allocated for the forest
is set at the beginning and it is shared by all
the trees.  Therefore, the memory footprint of the
classifier is constant.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

\paragraph{Naive Bayes~\cite{naive_bayes}}
The naive Bayes algorithm keeps a table of
counters for each feature's values and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase.

The implementation from StreamDM was used in this
benchmark. It uses a Gaussian
estimation for numeric attributes.

In a naïve Bayes classifier, the smoothing parameter is the
default counter given to an attribute value that
has not been seen. It is meant to avoid scores of zeros.
We do not use any smoothing since our datasets
only contain numeric
attributes. Therefore, as long as there is one data
point to train with, there will be a Gaussian
estimation for each attribute.

\paragraph{k-Nearest Neighbors}
\TG{did you use kNN at all? They don't seem to be in the results.}

The k-Nearest Neighbors~(kNN) is a classification
algorithm that uses the k nearest data points from
the training set to classify.  An adaption of this
algorithm to data stream classification is the use
of a sliding window~\cite{Mining_Massive_Datasets}
as the training set.

Normally, the sliding window stores the most
recent data point, but in this benchmark, we used
a biased window~\cite{biased_reservoir_sampling}.
This window acts as a sample of
the stream (similar to a reservoir sampling) but
biased on the most recent data point.  Therefore,
the window does not contain the most recent data
point, but instead, these data points will have a
higher chance of being in the window.  This type
of window is useful to keep knowledge from the
past while focusing on the recent part of the
dataset.

The kNN algorithm can be tuned by adjusting the
parameter k. It can also be tuned by changing the
distance function. Finally, it can be optimized by
adapting the window size.

\paragraph{Neural Network}
%We need a citation there
A neural network is a combination of artificial
neurons also known as perceptrons. Each
perceptron has weighted input values and an
activation function. To predict a class label, the
perceptron applies the activation function to the sum
of its input values. The output
value of the perceptron is the result of this
activation function. This prediction phase is also
called feed-forward. To train the neural network,
we feed-forward, then the error between the
prediction and the expected result is used in the
backpropagation process to adjust the weights of
the input values.  A neural network combines
multiple perceptrons by connecting perceptron outputs
to inputs of other perceptrons.  In
this benchmark, we used a Multi-Layer Perceptron, 
that is, a network where perceptrons are organized in
layers and all output
values from perceptrons of layer $n-1$ serve as
input values for perceptrons of layer $n$. 

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.

\paragraph{Hyperparameters Tuning}
Hyperparameters were tuned using the first
subject from the \banosdataset dataset.  The data from
this subject was pre-processed as the rest of
the \banosdataset dataset (window size of one second,
average and standard deviation on the three
acceleration axis of the right forearm sensor,
$\ldots$). We tested multiple values for the
parameters.  We did
not explore all the parameters at once, but
instead a small combination of them each at a
time. \TG{that's a bit vague. Did you do grid search?}

\subsection{Evaluation}
We used the prequential error to evaluate the algorithms, that is, we tested
algorithms with each new data point before training with it. When a
new label is encountered, we do \TG{fix switch between present and past} not test the algorithm with the data point, we
only train it. We did not used any fading factor in the prequential error, and we averaged
values from multiple runs \TG{what are runs? repetition with different random seeds? in different orders?}.

We measured the F1-score, the accuracy, and the memory footprint periodically
rather than after each data point, to reduce the computing load of the experiment.
We collected energy consumption at the end of each run, alongside average power.

%Time to process one element.
%Explain the concept drift recovery time.
\subsubsection{Classification Performance}
We used the accuracy and the F1-score to measure the classification
performances of the classifiers. These two metrics were computed every ten
elements to avoid overloading the run with this computation. We did not apply
any fading factor that could attenuate errors from the beginning.

We computed the
F1-score\footnote{\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{Code
of the F1-score}.} in a one-versus-all fashion for each class, then we took the
average.  When a class has not been encountered yet, its F1-score is ignored.
Similarly to the accuracy, we did not apply a fading factor.

\subsubsection{Memory}
We measured the memory footprint by reading the file "/proc/self/statm". This measure is
taken every 40 data points to avoid spending too much time reading the file.

\subsubsection{Energy}
We measured the energy consumption using rapl
tools\footnote{\url{https://github.com/kentcz/rapl-tools.git}}.  We used the
AppPowerMeter\footnote{\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/makefile.py\#L122}{Code
is the use of AppPowerMeter here}.} that measures the energy used by the
computer while an application is running then output the amount of Joules used
is output at the end of the run alongside the average power in Watts.

We measured the energy consumed by each classifier
multiple times in a minimal environment. We also added an empty
classifier that did not classify anything and always
returned zero, to serve as nominal energy consumption.

\subsection{Experimental Conditions}
Our experimental conditions are implemented in a Python script that defines
classifiers and their parameters, randomizes all the runs, and plots the
resulting data. The datasets and output results are stored in memory
through a memfs filesystem mounted on "/tmp". We proceed in this manner to
minimize the impact of I/O on classification time.

When collecting the data, we averaged metrics for multiple runs of the same
classifier (same classifier, same parameters, and same file). The energy
consumption is the average energy consumption of all runs. Regarding the
classification performance (F1-score, accuracy), we averaged measurements
accross repetition.

% vim: tw=50 ts=2

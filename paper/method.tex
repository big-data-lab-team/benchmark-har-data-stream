\section{Method}
We evaluated 6 classifiers implemented in
either StreamDM-Cpp~\cite{StreamDM-CPP} or
OrpailleCC~\cite{OrpailleCC}.  StreamDM-Cpp is a
C++ implementation of StreamDM~\cite{StreamDM}, a
software to mine big data streams using Spark
Streaming. StreamDM-Cpp is faster than
StreamDM on small scale, most likely because of
the overhead induced by Spark. However, StreamDM-Cpp makes heavy
use of the C standard library, in particular the vector and
list structures that may not be available on embedded systems
with limited OS functions \TG{This could be explained in intro rather than here.}.

OrpailleCC is a
collection of data stream algorithms developed to
be deployed on embedded devices. The key
functions, such as random number generation or memory allocation, are
parametrizable through the class template and can thus be customized 
on a given execution platform.
OrpailleCC is not limited to classification
algorithms, it implements other data stream
algorithms such as the Cuckoo filter~\cite{cuckoo}
or a multi-dimensional extension of the
Lightweight Temporal Compression~\cite{multi-ltc}.

This benchmark includes six popular algorithms.  The Mondrian
forest~\cite{mondrian2014} builds decision trees independently of class labels,
which are only used during classification. Mondrian forests are useful in
situations where labels are delayed, as described
in~\cite{stream_learning_review}.  The Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the K-nearest neighbor~(KNN).
It claims to be more efficient than its vanilla counterpart on human activity
recognition. Since a previous study~\cite{Janidarmian_2017} showed that a
K-nearest neighbor~(KNN) would achieve the best classification performance on
human activity recognition with data acquired by wearable sensors, we decided
to include this alternative to the study.  The Naïve Bayes~\cite{naive_bayes}
and the Hoeffding Tree~\cite{VFDT} are two common algorithms in classification
and data stream classification.  Finally, Neural Network classifiers have
become popular by achieving human-like performance in many fields such as image
recognition or game playing. For this study, we decided to explore the use of
Multi-Layer Perceptron with one hidden layer, as adopted in~\cite{omid_2019}.
This paper tested the Multi-Layer Perceptron on a low-power platform to
recognize fitness activity and it achieved an
average F1-score of $0.95$. They used one hidden
layer with the sigmoid activation except for the
output which uses the softmax.

The remainder of this section describes the datasets, classifiers,
evaluation metrics and parameters used in our benchmark.

\subsection{Datasets}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a human
activity dataset with 17 participants
equipped with 9 sensors. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as well as 
the orientation in a quaternion format, producing a total of 13 values.
Sensors are
sampled at 50 Hz and each sample is associated
with one of 33 activities. In
addition to the 33 activities, an extra activity
labelled 0 indicates no specific activity.

We pre-processed the \banosdataset using
non-overlapping windows of one second (50
samples), and using only the 3 acceleration axes of the
right forearm sensor. From each axis, features were computed as the
average and the standard deviation over the window. The label assigned to this data
point was the most frequent label in the window.
The resulting data points were shuffled
across subjects.

From the \banosdataset, we derived another dataset
that involve a drift. After half of the data
points, we shifted the activity labels.

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset obtained with 94
participants. Similarly to the \banosdataset
dataset, the activity labelled 0 indicates no
specific activity.
Since many of these activities are similar, we
merged some of them together. 
\MK{no paper on that, just Omid report.}

We pre-processed the dataset similarly to the
\banosdataset, using non-overlapping windows of
one second, and only using the 3 acceleration axes
of the 6 axis data. From these 3 axis, we computed
features as the average and the standard deviation
over the window. The label assigned to this data
point was the most frequent label in the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We used these three commands to
generate three synthetic datasets\footnote{MOA commands available
\href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}}:
a hyperplane, a RandomRBF, and a RandomTree
dataset. We generated 20,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF have both three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three. It has six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.
\TG{You should put this dataset in the paper repo, and \banosdataset too if the license allows}

\subsection{Algorithm and Implementation}
\paragraph{Hoeffding Tree~\cite{VFDT}}
%\begin{itemize}
	%\item Reserved size with a given size.
	%\item Binary tree.
	%\item Focus on real numbers features.
	%\item The number of split considered by features is given by the user.
	%\item Split are determined by forming a boxes and spliting these boxes.
	%\item Majority vote at the leaves.
	%\item All floating point values are double and all counters are int.
%\end{itemize}
Similar to a decision tree, the Hoeffding tree recursively splits the space to maximize a
metric function, often the
information gain or the Gini index. However,
instead of using the entire dataset, the Hoeffding
tree relies on the Hoeffding bound to estimate
when a leaf should be split. When a prediction is required on a
data point, this data point is sorted to a leaf,
then the training data points in that leaf are
used to classify. The most common way is to apply a
majority vote or a Naïve Bayes.

This classifier is implemented in StreamDM.

This algorithm is common in data stream
classification, however, it suffers from one main
disadvantage: once a split is decided, it cannot
be re-considered which makes this algorithm weak
to concept drift.

\paragraph{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(MCNN) is a
variant of k-nearest neighbor were data points are
aggregated into clusters.  During training, the
algorithm merges a new data point to its closest
cluster that shares the same label. If the closest
cluster does not share the same label as the data
point, this closest cluster and the closest
cluster with the same label as the data point
receive an error. When a cluster have received too
many errors, it is split. During classification,
MCNN returns the label of the closest cluster.
Regularly, the algorithm also assigns a
participation score to each cluster and when this
score gets below a threshold, the cluster is
removed. Given that the maximum number of clusters
is fixed, this mechanism helps to make space for
new clusters in the future.  

We implemented MCNN in OrpailleCC. To save computation time, we decided to remove the
participation threshold mechanism and to remove the
cluster with the lowest participation
when space is needed. 

The MCNN algorithm can be tuned using two
parameters: the maximum number of clusters and the
error threshold after which a cluster is split.


\paragraph{Mondrian Forest~\cite{mondrian2014}}
Each tree in a Mondrian forest recursively
splits the space, as a regular decision tree.
However, the feature to split and the value of the
split are decided randomly. The probability to select a feature is 
proportional to its normalized range. The value for the split is
uniformly selected in the range of the feature. When asked to predict, a node
combine its own label observation and its parent prediction.

In OrpailleCC, the size allocated for the forest
is set at the beginning and it is shared by all
the trees.  Therefore, the memory footprint of the
classifier is constant.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

\paragraph{Naive Bayes~\cite{naive_bayes}}
The Naive Bayes algorithm keeps a table of
counters for each feature's values and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase.

The implementation from StreamDM was used in this
benchmark. It uses a Gaussian
estimation for numeric attributes.

In a Naïve Bayes classifier, the smoothing is the
default counter given to an attribute value that
have not been seen to avoid score of zeros.
We do not use any smoothing since the datasets
we are focusing on only contain numeric
attributes. Therefore, as long as there is one data
point to train with, there will be a Gaussian
estimation for each attribute.

\paragraph{k-Nearest Neighbors}
The k-Nearest Neighbors~(kNN) is a classification
algorithm that uses the k nearest data points from
the training set to classify.  An adaption of this
algorithm to data stream classification is the use
of a sliding window~\cite{Mining_Massive_Datasets}
as the training set.

Normally, the sliding window stores the most
recent data point, but in this benchmark, we used
a biased window~\cite{biased_reservoir_sampling}.
This window acts as a sample of
the stream (similar to a reservoir sampling) but
biased on the most recent data point.  Therefore,
the window does not contain the most recent data
point, but instead, these data points will have a
higher chance of being in the window.  This type
of window is useful to keep knowledge from the
past while focusing on the recent part of the
dataset.

The kNN algorithm can be tuned by adjusting the
parameter k. It can also be tuned by changing the
distance function. Finally, it can be optimized by
adapting the window size.

\paragraph{Neural Network}
%We need a citation there
A neural network is a combination of artificial
neurons also known as perceptrons. Each
perceptron has weighted input values and an
activation function. To predict a class label, the
perceptron applies the activation function to the sum
of its input values. The output
value of the perceptron is the result of this
activation function. This prediction phase is also
called feed-forward. To train the neural network,
we feed-forward, then the error between the
prediction and the expected result is used in the
backpropagation process to adjust the weights of
the input values.  A neural network combines
multiple perceptrons by connecting perceptron outputs
to inputs of other perceptrons.  In
this benchmark, we used a Multi-Layer Perceptron, 
that is, a network where perceptrons are organized in
layers and all output
values from perceptrons of layer $n-1$ serve as
input values for perceptrons of layer $n$. 

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.

\paragraph{Hyperparameters Tuning}
Hyperparameters were tuned using the first
subject from the \banosdataset dataset.  The data from
this subject received the same transformation as
the \banosdataset (window size of one second,
average and standard deviation on the three
acceleration axis of the right forearm sensor,
$\cdots$). We tested multiple values for the
parameters.  We did
not explore all the parameters at once, but
instead a small combination of them each at a
time.

\subsection{Evaluation}
We used the prequential error to evaluate the algorithms. Thus, we tested
algorithms with each new data point before training with it. Note that when a
new label is encountered, we do not test the algorithm with the data point, we
solely train it. We did not apply a fading factor on the metric. We aggregated
metrics from multiple runs with the average.

We measured the F1-score, the accuracy, and the memory footprint periodically
rather than after each data point. It aleviates the computing load without
changing the final results.
We collected the energy metric at the end of each run, alongside the power metric.

%Time to process one element.
%Explain the concept drift recovery time.
\subsubsection{Classification Performance}
We used the accuracy and the F1-score to measure the classification
performances of the classifiers. These two metrics were computed every ten
elements to avoid overloading the run with this computation. We did not apply
any fading factor that could attenuate errors from the beginning.

We computed the
F1-score\footnote{\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{Code
of the F1-score}.} in a one-versus-all fashion for each class, then we took the
average.  When a class has not been encountered yet, its F1-score is ignored.
Similarly to the accuracy, we did not apply a fading factor.

\subsubsection{Memory}
We measured the memory footprint by reading the file "/proc/self/statm". This measure is
taken every 40 data points to avoid spending too much time reading the file.

\subsubsection{Energy}
We measured the energy consumption using rapl
tools\footnote{\url{https://github.com/kentcz/rapl-tools.git}}.  We used the
AppPowerMeter\footnote{\href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/makefile.py\#L122}{Code
is the use of AppPowerMeter here}.} that measures the energy used by the
computer while an application is running then output the amount of Joules used
is output at the end of the run alongside the average power in Watts.

We measured the energy consumed by each classifier
multiple times in a minimal environment. We also added an empty
classifier that did not classify anything and always
returned zero, to serve as nominal energy consumption.

\subsection{Experimental Conditions}
Our experimental conditions are implemented in a Python script that defines
classifiers and their parameters, randomizes all the runs, and plots the
resulting data. The datasets and output results are stored in memory
through a memfs filesystem mounted on "/tmp". We proceed in this manner to
minimize the impact of I/O on classification time.

When collecting the data, we averaged metrics for multiple runs of the same
classifier (same classifier, same parameters, and same file). The energy
consumption is the average energy consumption of all runs. Regarding the
classification performance (F1-score, accuracy), we averaged measurements
accross repetition.

% vim: tw=50 ts=2

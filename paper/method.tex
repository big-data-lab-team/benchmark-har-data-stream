\section{Method}
We evaluated 6 classifiers implemented in either
StreamDM-Cpp~\cite{StreamDM-CPP} or
OrpailleCC~\cite{OrpailleCC}.  StreamDM-Cpp is a
C++ implementation of StreamDM~\cite{StreamDM}, a
software to mine big data streams using
\href{https://spark.apache.org/streaming/}{Apache
Spark Streaming}. StreamDM-Cpp is usually faster
than StreamDM on small datasets, due to the
overhead induced by Spark, but it makes heavy use
of the C standard library, in particular the
vector and list structures, which may not be
available on embedded systems \TG{This should be
explained in intro rather than here.}.

OrpailleCC is a collection of data stream
algorithms developed for embedded devices. The key
functions, such as random number generation or
memory allocation, are parametrizable through
class templates and can thus be customized on a
given execution platform.  OrpailleCC is not
limited to classification algorithms, it
implements other data stream algorithms such as
the Cuckoo filter~\cite{cuckoo} or a
multi-dimensional extension of the Lightweight
Temporal Compression~\cite{multi-ltc}. We extended
it with a few classifiers for the purpose of this
benchmark.

This benchmark includes six popular classification
algorithms.  The
\mondrianforest~\cite{mondrian2014} builds
decision trees without immediat need of labels
which is useful in situations where labels are
delayed, as described
in~\cite{stream_learning_review}\TG{reworded this
but it's still a bit ambiguous}.  The
Micro-Cluster Nearest
Neighbors~\cite{mc-nn} is a compressed version of the K-nearest
neighbor~(KNN) that was shown to be more accurate for human activity
recognition \TG{reference?}; we included
it since the work in~\cite{Janidarmian_2017} showed that KNN was the
best-performing classifier for human activity recognition from wearable
sensors.  The \naivebayes~\cite{naive_bayes}
classifier builds a table of feature
occurrence to estimate class
likelihoods.
The \hoeffdingtree~\cite{VFDT} builds a
decision tree using the Hoeffding Bound to
estimate when the best split is found. 
Finally, Neural Network classifiers have
become popular by achieving human-like performance in many fields such as image
recognition or game playing. We used a
\FNN with one hidden layer, as described in~\cite{omid_2019} for the recognition 
of fitness activities on a low-power platform.



The remainder of this section details the datasets, classifiers,
evaluation metrics and parameters used in our benchmark.

\subsection{Datasets}
\subsubsection{\banosdataset}
%50 Hz sampling.
%117 data per sample.
%33 activities.
%Sensors cover the body.
%Type of data (ideal or self)
%17 subject.
The \banosdataset dataset~\cite{Banos_2014} is a
human activity dataset with 17 participants
and 9 sensors per participant. Each sensor samples a 3D
acceleration, gyroscope, and magnetic field, as
well as the orientation in a quaternion format,
producing a total of 13 values.  Sensors are
sampled at 50~Hz, and each sample is associated
with one of 33 activities. In addition to the 33
activities, an extra activity labelled 0 indicates
no specific activity.

We pre-processed the \banosdataset dataset using
non-overlapping windows of one second (50
samples), and using only the 6 acceleration axes
of the right forearm sensor. We computed the average and the standard deviation over the
window as features for each axis. We assigned the most
frequent label to the window.  The resulting data
points were shuffled uniformly.

In addition, we constructed another dataset from \banosdataset, in which we
simulated a concept drift by shifting the activity labels in the
second half of the data stream. This is useful to
observe any behavioral change induced by the
concept drift such as an increase in power
consumption.

\subsubsection{\recofitdataset}
The \recofitdataset dataset~\cite{recofit} is a
human activity dataset containing 94
participants. Similarly to the \banosdataset
dataset, the activity labelled 0 indicates no
specific activity.
Since many of these activities are similar, we
merged some of them together based on the table
in~\cite{behzad2019}. 

We pre-processed the dataset similarly to the
\banosdataset one, using non-overlapping windows of
one second, and only using the 6 acceleration axes
of the 6 axis data. From these 6 axes, we used the average and the standard deviation
over the window as features. We assigned the most
frequent label to the window.

\subsubsection{MOA dataset}
Massive Online Analysis~\cite{moa} (MOA) is a Java framework to compare
data stream classifiers. In addition to classification algorithms, MOA provides many
tools to read and generate datasets.
We generated three synthetic datasets\footnote{MOA commands available \href{https://github.com/azazel7/paper-benchmark/blob/e0c9a94d0d17490f7ab14293dec20b8322a6447c/Makefile\#L90}{here}}:
a hyperplane, a RandomRBF, and a RandomTree
dataset \TG{juste pour mon info: tu sembles
regenerer les datasets a chaque run, utilises-tu
la meme graine a chaque fois?}. We generated 200,000 data points
 for each of these synthetic datasets.
The hyperplane and the RandomRBF both have three features and two classes, however, the RandomRBF has a slight imbalance toward one class.
The RandomTree dataset is the hardest of the three, with six attributes and
ten classes. Since the data points are generated with a tree structure, we
expect the decision trees to show better performances than the other
classifiers.
\TG{You should put this dataset in the paper repo, and \banosdataset too if the license allows}

\subsection{Algorithms and Implementation}
In this section, we describe the algorithms used in the benchmark, their
hyperparameters, and relevant implementation details. 

\subsubsection{\mondrianforest~\cite{mondrian2014}}
Each tree in a \mondrianforest recursively splits
the feature space, similar to a regular decision tree.
However, the feature used in the split and the
value of the split are picked randomly. The
probability to select a feature is proportional to
its normalized range. The value for the split is
uniformly selected in the range of the feature.
During prediction, a node combines its observed
label count with its parent prediction.

In OrpailleCC, the size allocated for the forest
is set at the beginning and it is shared by all
the trees.  Therefore, the memory footprint of the
classifier is constant.

Mondrian trees can be tuned using three
parameters: the base count, the discount factor,
and the budget. The base count is used to
initialize the prediction for the root. The
discount factor influences the nodes on how much
they should use their parent prediction. A
discount factor closer to one makes the prediction
of a node closer to the prediction of its parent.
Finally, the budget controls the tree depth.

Table~\ref{table:hyperparameter-mondrian} shows
the hyperparameters used for each number of trees.
\begin{figure}
	\begin{center}
		\begin{tabular}{|| r | c | c | c ||} 
			\hline
			Number of trees &  Base count & Discount & Budget \\ [0.5ex] 
			\hline\hline
			1 & 0.1 & 1.0 & 0.8 \\
			5 & 0.0 & 1.0 & 0.8 \\
			10 & 0.0 & 1.0 & 0.6 \\
			50 & 0.0 & 0.1 & 0.6 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Hyperparameters used for \mondrianforest.}
		\label{table:hyperparameter-mondrian}
\end{figure}


\subsubsection{Micro Cluster Nearest Neighbor~\cite{mc-nn}}
The Micro Cluster Nearest Neighbor~(\mcnn) is a
variant of k-nearest neighbors where data points
are aggregated into clusters to reduce storage
requirements.  During training, the algorithm
merges a new data point to the closest cluster
that shares the same label. If the closest cluster
does not share the same label as the data point,
this closest cluster and the closest cluster with
the same label are assigned an error. When a
cluster receives too many errors, it is split.
During classification, \mcnn returns the label of
the closest cluster.  Regularly, the algorithm
also assigns a participation score to each cluster
and when this score gets below a threshold, the
cluster is removed. Given that the maximum number
of clusters is fixed, this mechanism helps to make
space for new clusters in the future, and possibly
to adjust to concept drifts.  
The space and time complexity of \mcnn are
constant since the maximum number of cluster does
not change.

We implemented two versions of \mcnn in
OrpailleCC, which differ in the way they remove
clusters during training. The first version (\mcnn
Origin) is similar to the mechanism described
in~\cite{mc-nn}, based on participation scores.
Instead, the second version (\mcnn OrpailleCC)
removes the cluster with the lowest participation
only when space is needed.  A cluster slot is
needed when an existing cluster is split and there
are no more slot available because the number of
active cluster already reached the maximum defined
by the user.

The \mcnn OrpailleCC implementation has one
parameter, the error threshold after which a
cluster is split.  The \mcnn Origin has two
parameters: the error threshold and the
participation threshold. The participation
threshold is the limit below which a cluster is
removed.

Table~\ref{table:hyperparameter-mcnn} shows
the hyperparameters used for each number of clusters.
\begin{figure}
		\begin{center}
			\begin{tabular}{|| r | c ||} 
				\hline
				Number of clusters &  error threshold \\ [0.5ex] 
				\hline\hline
				10 & 16 \\
				20 & 4 \\
				33 & 10 \\
				40 & 4 \\
				50 & 2 \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Hyperparameters used for \mcnn.}
		\label{table:hyperparameter-mcnn}
\end{figure}

\subsubsection{k-Nearest Neighbors}
\TG{did you use kNN at all? They don't seem to be in the results.}

The k-Nearest Neighbors~(kNN) is a classification
algorithm that uses the k nearest data points from
the training set to classify.  An adaption of this
algorithm to data stream classification is the use
of a sliding window~\cite{Mining_Massive_Datasets}
as the training set.

Normally, the sliding window stores the most
recent data point, but in this benchmark, we used
a biased window~\cite{biased_reservoir_sampling}.
This window acts as a sample of
the stream (similar to a reservoir sampling) but
biased on the most recent data point.  Therefore,
the window does not contain the most recent data
point, but instead, these data points will have a
higher chance of being in the window.  This type
of window is useful to keep knowledge from the
past while focusing on the recent part of the
dataset.

The kNN algorithm can be tuned by adjusting the
parameter k. It can also be tuned by changing the
distance function. Finally, it can be optimized by
adapting the window size.

\subsubsection{\naivebayes~\cite{naive_bayes}}
The \naivebayes algorithm keeps a table of
counters for each feature's values and each label.
During prediction, the algorithm assigns a
score for each label depending on how the data
point to predict compares to the values observed
during the training phase.

The implementation from StreamDM was used in this
benchmark. It uses a Gaussian
fit for numeric attributes.

In a \naivebayes classifier, the smoothing parameter is the
default counter given to an attribute value that
has not been seen. It is meant to avoid scores of zeros.
We do not use any smoothing since our datasets
only contain numeric
attributes. Therefore, as long as there is one data
point to train with, there will be a Gaussian
fit for each attribute.

\subsubsection{\hoeffdingtree~\cite{VFDT}}
%\begin{itemize}
	%\item Reserved size with a given size.
	%\item Binary tree.
	%\item Focus on real numbers features.
	%\item The number of split considered by features is given by the user.
	%\item Split are determined by forming a boxes and spliting these boxes.
	%\item Majority vote at the leaves.
	%\item All floating point values are double and all counters are int.
%\end{itemize}
Similar to a decision tree, the \hoeffdingtree
recursively splits the feature space to maximize a metric, often the
information gain or the Gini
index. However, instead of using the entire
dataset, the \hoeffdingtree relies on the
Hoeffding bound, a measure of the score deviation
of the splits, to estimate when a leaf should be
split. During classification, a data point
is sorted to a leaf, and a label is predicted by
aggregating the labels of the training data points
in that leaf, usually through majority voting or
\naivebayes classification.  We used this
classifier as implemented in StreamDM-Cpp.  The
\hoeffdingtree is common in data stream
classification, however, it suffers from one main
disadvantage: once a split is decided, it cannot
be re-considered which makes this algorithm weak
to concept drifts.

The \hoeffdingtree has multiple parameters: the
confidence level, the grace period, and the leaf
learner. The confidence level is probability of
the Hoeffding bound to make a wrong estimation of
the deviation. The grace period is the number of
processed data points before a leaf is evaluated for split.
 The leaf learner is the method used in the
leaf to predict the label.  In this study we used
a confidence level of $0.01$ with a grace period
of 10 data points and \naivebayes as leaf
learners.

\subsubsection{\FNN}
%We need a citation there
A neural network is a combination of artificial
neurons, also known as perceptrons, that all have input weights and an
activation function. To predict a class label, the
perceptron applies the activation function to the weighted sum
of its input values. The output
value of the perceptron is the result of this
activation function. This prediction phase is also
called feed-forward. To train the neural network,
feed-forward is applied first, then the error between the
prediction and the expected result is used in the
backpropagation process to adjust the weights of
the input values.  A neural network combines
multiple perceptrons by connecting perceptron outputs
to inputs of other perceptrons.  In
this benchmark, we used a fully-connected \FNN, 
that is, a network where perceptrons are organized in
layers and all output
values from perceptrons of layer $n-1$ serve as
input values for perceptrons of layer $n$. 
We used a 3-layer network with 120 inputs, 30
perceptrons in the hidden layer, and 33 output
perceptrons.

In this study, we used histogram features
from~\cite{omid_2019} instead of the ones
presented in Section~\ref{sec:method-dataset}
because the network performed
poorly with these features. The histogram features
produce 20 bins per axis.

This neural network can be tuned by changing the
number of layers and the size of each layer.
Additionally, the activation function and the
learning ratio can be changed. The learning ratio
indicates by how much the weights should change
during backpropagation.


\subsubsection{Hyperparameters Tuning}
Hyperparameters were tuned using the first
subject from the \banosdataset dataset.  The data from
this subject was pre-processed as the rest of
the \banosdataset dataset (window size of one second,
average and standard deviation on the three
acceleration axis of the right forearm sensor,
$\ldots$). We tested multiple values for the
parameters.

The
classifiers start the prequential phase with no
knowledge from the first subject, except the \FNN \TG{why?}. Only the
hyperparameters were selected to maximize the
F1-score \TG{unclear, should be rephrased}.

We noticed that the
\FNN performs very pooly without a
pretraining of many epochs. Otherwise, it 
does not learn fast enough and it does not do better than a
random classifier. Therefore, we decided to
pre-train the \FNN and re-use
the weights as a starting point for the
prequential phase.

\subsection{Evaluation}
We computed four metrics: the F1-score, the memory
footprint, the runtime, and the power usage \TG{is accuracy missing in this list, or should it be removed from the remainder of the text?}.
The F1-score, the accuracy, and the memory
footprint were computed periodically during the
execution of a classifier. The
power consumption and the runtime were collected
at the end of each execution.

%Time to process one element.
%Explain the concept drift recovery time.
\paragraph{Classification Performance}
We used the prequential error to evaluate the
algorithms, that is, we tested algorithms with
each new data point before training with it.  We
monitored the accuracy and the F1-score to measure
the classification performances of the
classifiers. These two metrics were computed every
50
elements to avoid overloading the run with this computation \TG{so you
measure the prequential error (TP, FP, TN, FN for each class) after each
element but you compute the F1 score only every 10 elements, right?}. We
did not apply any fading factor that could attenuate errors from the
beginning.
We computed the
F1-score in a one-versus-all fashion for each class, averaged across all classes
(macro-average, code available \href{https://github.com/azazel7/paper-benchmark/blob/9adb1039c5a65a00a66d554f0e870d14d3fff7cb/main.cpp\#L82}{here}).  When a class has not been encountered yet, its F1-score is ignored.
Similarly to the accuracy, we did not apply a fading factor.

\paragraph{Memory}
We measured the memory footprint by reading file
\texttt{/proc/self/statm} every 50 data points.

\paragraph{Runtime}
The runtime of a classifier is the time needed for
the classifier to process the dataset. It is
measured at the end of an execution using the
\textit{perf}
command.

%\MK{Explain the runtime somewhere and how it is
%computed.}

\paragraph{Power} We measured the average power
consumed by classification algorithms using the
\textit{perf} command. We measured the power used by each
classifier multiple times in a minimal
environment. To provide a baseline power usage, we
also added an empty classifier that always
predicted class 0.

\subsection{Experimental Conditions}
We automated our experiments with a Python script that defines
classifiers and their parameters, randomizes all the runs, and plots the
resulting data. The datasets and output results are stored in memory
through a memfs filesystem mounted on \texttt{/tmp}, to reduce the impact of I/O time.

We averaged metrics for multiple runs of the same
classifier (same classifier, same parameters, and
same dataset). The energy
consumption is the average energy consumption of
the classifier. Regarding the
classification performance (F1-score, accuracy), we averaged measurements
across repetitions \TG{are these repetitions the same as runs that are averaged at the beginning of section II.C? If so, 
I would remove the mentioning of averaging there, and only put it here.}.

The experiment was done  with a single core of a
cluster node. The node was two Intel(R) Xeon(R)
Gold 6130 CPU and a main memory of 250G.

% vim: tw=50 ts=2
